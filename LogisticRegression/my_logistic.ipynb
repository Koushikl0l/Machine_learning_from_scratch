{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "327a65ab-9685-40a9-919a-a4b3c2d78a94",
   "metadata": {},
   "source": [
    "<h4>Implementing logistic regression from scratch in Python</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98630b53-64e8-4e0b-91d2-e63f249cf877",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ddf651-e4c7-42a1-8928-f0b6927aeea2",
   "metadata": {},
   "source": [
    "<p>Logistic regression is often mentioned in connection to classification tasks. The model is simple and one of the easy starters to learn about generating probabilities, classifying samples, and understanding gradient descent. This tutorial walks you through some mathematical equations and pairs them with practical examples in Python so that you can see exactly how to train your own custom binary logistic regression model.</p>\n",
    "\n",
    "<h4>First we Initialize some parameters</h4>\n",
    "<h5>In machine learning we call those parameters ,hyperparameter</h5>\n",
    "\n",
    "<ul>\n",
    "  <li>learning_rate</li>\n",
    "  <li>Iteration</li>\n",
    "  <li>weight :if not scratch this param generate automatically in Keras/Tensorflow</li>\n",
    "  <li>bias : if not scratch this param generate automatically in Keras/Tensorflow</</li>\n",
    "</ul> \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c846951f-c7a8-4f0c-9c5b-d3920f620358",
   "metadata": {},
   "outputs": [],
   "source": [
    "def __init__(self, learning_rate=0.001, n_iters=1000):\n",
    "        self.lr = learning_rate\n",
    "        self.n_iters = n_iters\n",
    "        self.weights = None\n",
    "        self.bias = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29863700-2094-44cf-8eea-a223c500b2a8",
   "metadata": {},
   "source": [
    "<h4>Why Sigmoid ,why not linear function?</h4>\n",
    "<h5>However, a linear activation function has two major problems :</h5>\n",
    "<ul>\n",
    "  <li>It’s not possible to use backpropagation as the derivative of the function is a constant and has no relation to the input x.</li>\n",
    "  <li>All layers of the neural network will collapse into one if a linear activation function is used. No matter the number of layers in the neural network, the last layer will still be a linear function of the first layer. So, essentially, a linear activation function turns the neural network into just one layer.</li>\n",
    "  \n",
    "</ul> \n",
    "\n",
    "<h4>Non Linear Activation</h4>\n",
    "<p>The linear activation function shown above is simply a linear regression model. \n",
    "Because of its limited power, this does not allow the model to create complex mappings between the network’s inputs and outputs. \n",
    "</p>\n",
    "<p>Non-linear activation functions solve the following limitations of linear activation functions:</p>\n",
    "\n",
    "<ul>\n",
    "  <li>They allow backpropagation because now the derivative function would be related to the input, and it’s possible to go back and understand which weights in the input neurons can provide a better prediction.</li>\n",
    "  <li>They allow the stacking of multiple layers of neurons as the output would now be a non-linear combination of input passed through multiple layers. Any output can be represented as a functional computation in a neural network.\n",
    "</li>\n",
    "  \n",
    "</ul> \n",
    "<h5>***Reference</h5>\n",
    "<a href=\"https://timvieira.github.io/blog/post/2014/02/11/exp-normalize-trick/\">Exp Normalization Trick: Numerically stable sigmoid function\n",
    "\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "9c403847-390e-45b1-a35a-e9243366658b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _sigmoid(x):\n",
    "    \"Numerically stable sigmoid function.\"\n",
    "    if x >= 0:\n",
    "        z = np.exp(-x)\n",
    "        return 1 / (1 + z)\n",
    "    else:\n",
    "        # if x is less than zero then z will be small, denom can't be\n",
    "        # zero because it's 1+z.\n",
    "        z = np.exp(x)\n",
    "        return z / (1 + z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "cc64d6ab-6582-4136-a0f2-ee43623f6247",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid( x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "cc430ae9-2b26-43ff-bd79-67cc96508fdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0, 1.0)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid(np.inf),_sigmoid(np.inf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d676287b-4865-4691-8515-70e123665c71",
   "metadata": {},
   "source": [
    "<center><img src=\"sigmoid.png\" alt=\"Girl in a jacket\" width=\"800\" height=\"700\"></center>\n",
    "<p>This function takes any real value as input and outputs values in the range of 0 to 1. \n",
    "The larger the input (more positive), the closer the output value will be to 1.0, whereas the smaller the input (more negative), the closer the output will be to 0.0, as shown below.</p>\n",
    "<p>It is commonly used for models where we have to predict the probability as an output. Since probability of anything exists only between the range of 0 and 1, sigmoid is the right choice because of its range.</p>\n",
    "\n",
    "<ul>\n",
    "  <li>The derivative of the function is f'(x) = sigmoid(x)*(1-sigmoid(x)).</li>\n",
    "\n",
    "</li>\n",
    "  \n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a1545b-1048-451d-8334-8321ff776c15",
   "metadata": {},
   "source": [
    "<h4>Cross Entropy Loss</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "2d577675-d35e-48d4-a848-9115d32d6035",
   "metadata": {},
   "outputs": [],
   "source": [
    " def compute_loss(self, y_true, y_pred):\n",
    "        # binary cross entropy\n",
    "        epsilon = 1e-9\n",
    "        y1 = y_true * np.log(y_pred + epsilon)\n",
    "        y2 = (1-y_true) * np.log(1 - y_pred + epsilon)\n",
    "        return -np.mean(y1 + y2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44db48d-eb22-4428-a2bd-204074de871f",
   "metadata": {},
   "source": [
    "<center><img src=\"bce.png\" alt=\"Girl in a jacket\" width=\"800\" height=\"700\"></center>\n",
    "<center>Binary Cross Entropy Loss</center>\n",
    "</br>\n",
    "<p>  It’s a method of evaluating how well your algorithm models your dataset. If your predictions are totally off, your loss function will output a higher number. If they’re pretty good, it’ll output a lower number. As you change pieces of your algorithm to try and improve your model, your loss function will tell you if you’re getting anywhere.You are essentially finding all of the errors by comparing your ground truth y_true to your predictions y_pred (also known as y hat from your explanation section).</p>\n",
    "\n",
    "<h5>***Reference</h5>\n",
    "<a href='https://www.deeplearningbook.org/'>Deep Learning, by Ian Goodfellow, Yoshua Bengio and Aaron Courville.</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28b3e5b-2b48-4bf0-b50f-e20559386f30",
   "metadata": {},
   "source": [
    "<h4>Gradient Descent and  (Forward + BackProp)</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd810be-e007-45b4-b9ad-c8581976d608",
   "metadata": {},
   "source": [
    "<center><img src=\"gradient.png\" alt=\"Girl in a jacket\" width=\"800\" height=\"700\"></center>\n",
    "<p>Gradient Descent is an algorithm that is used to optimize the cost function or the error of the model. It is used to find the minimum value of error possible in your model. Gradient Descent can be thought of as the direction you have to take to reach the least possible error.</p>\n",
    "<p>So here’s the definition: when you derive a function, you get an equation that tells you what the gradient of your function will be at any given value for x.\n",
    "Looking back at the graph of the cost function, if we could therefore derive the cost function, we could find out what the gradient is.\n",
    "</p>\n",
    "<p>Here 'w' sets the direction and 'b' move the direction back and forth.</p>\n",
    "\n",
    "<p>***References</p>\n",
    "\n",
    "<a href='https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=&cad=rja&uact=8&ved=2ahUKEwjY0sbh1v6AAxUd4TgGHYPHA_4QFnoECEYQAQ&url=https%3A%2F%2Fsee.stanford.edu%2Fmaterials%2Faimlcs229%2Fcs229-notes1.pdf&usg=AOvVaw13wb_XQus1qM1TAt6a8xnp&opi=89978449'>Stanford CS229.2018,Andrew Ng</a>\n",
    "\n",
    "<a href='https://www.youtube.com/watch?v=4b4MUYve_U8'>Stanford Lecture :CS229</a>\n",
    "\n",
    "<a href='https://www.deeplearningbook.org/'>Deep Learning  by Ian Goodfellow, Yoshua Bengio and Aaron Courville.</a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b66a5fe6-599f-4c70-8636-79a508ef885d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        # init parameters\n",
    "        self.weights = np.zeros(n_features)\n",
    "        self.bias = 0\n",
    "\n",
    "        # gradient descent\n",
    "        for _ in range(self.n_iters):\n",
    "            A = self.feed_forward(X)\n",
    "            dz = A - y # derivative of sigmoid and bce X.T*(A-y)\n",
    "\n",
    "            # compute gradients\n",
    "            dw = (1 / n_samples) * np.dot(X.T, dz)\n",
    "            db = (1 / n_samples) * np.sum(A - y)\n",
    "            # update parameters\n",
    "            self.weights -= self.lr * dw\n",
    "            self.bias -= self.lr * db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a73f44-4892-41d6-8de7-3e412092c01f",
   "metadata": {},
   "source": [
    "<h4>Here is the full code:</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4791365d-be2d-453e-b8d9-45678288c43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    def __init__(self, learning_rate=0.001, n_iters=1000):\n",
    "        self.lr = learning_rate\n",
    "        self.n_iters = n_iters\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "         \n",
    "    #Sigmoid method\n",
    "    def _sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def compute_loss(self, y_true, y_pred):\n",
    "        # binary cross entropy\n",
    "        epsilon = 1e-9\n",
    "        y1 = y_true * np.log(y_pred + epsilon)\n",
    "        y2 = (1-y_true) * np.log(1 - y_pred + epsilon)\n",
    "        return -np.mean(y1 + y2)\n",
    "\n",
    "    def feed_forward(self,X):\n",
    "        z = np.dot(X, self.weights) + self.bias\n",
    "        A = self._sigmoid(z)\n",
    "        return A\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        # init parameters\n",
    "        self.weights = np.zeros(n_features)\n",
    "        self.bias = 0\n",
    "\n",
    "        # gradient descent\n",
    "        for _ in range(self.n_iters):\n",
    "            A = self.feed_forward(X)\n",
    "            dz = A - y # derivative of sigmoid and bce X.T*(A-y)\n",
    "\n",
    "            # compute gradients\n",
    "            dw = (1 / n_samples) * np.dot(X.T, dz)\n",
    "            db = (1 / n_samples) * np.sum(A - y)\n",
    "            # update parameters\n",
    "            self.weights -= self.lr * dw\n",
    "            self.bias -= self.lr * db\n",
    "            \n",
    "    def predict(self, X):\n",
    "        y_hat = np.dot(X, self.weights) + self.bias\n",
    "        y_predicted = self._sigmoid(y_hat)\n",
    "        y_predicted_cls = [1 if i > 0.5 else 0 for i in y_predicted]\n",
    "        \n",
    "        return np.array(y_predicted_cls)\n",
    "\n",
    "    def accuracy(self,y, y_hat):\n",
    "        accuracy = np.sum(y == y_hat) / len(y)\n",
    "        return accuracy\n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6632917-a451-4036-9f74-7f190b4ac6dc",
   "metadata": {},
   "source": [
    "<h4>Output of the Regression</h4>\n",
    "<p>Here we use sklearn toy dataset (Brest Cancer) to test our regression model.As compare to Sklearn Logistic Resression model it gives a good output with 93% sccuracy with 91.7% precision.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6d55d90a-0983-433c-8f3b-e6e09ef7c140",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.930\n",
      "Confusion Matrix: [[39  6]\n",
      " [ 2 67]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    "\n",
    "dataset = datasets.load_breast_cancer()\n",
    "X, y = dataset.data, dataset.target\n",
    "\n",
    "X, y = dataset.data, dataset.target \n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=1234\n",
    ")\n",
    "\n",
    "regressor = LogisticRegression(learning_rate=0.0001, n_iters=1000)\n",
    "regressor.fit(X_train, y_train)\n",
    "predictions = regressor.predict(X_test)\n",
    "cm ,accuracy,sens,precision,f_score  = confusion_matrix(np.asarray(y_test), np.asarray(predictions))\n",
    "print(\"Test accuracy: {0:.3f}\".format(accuracy))\n",
    "print(\"Confusion Matrix:\",np.array(cm))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344d2611-c5a0-407e-9155-737779a39968",
   "metadata": {},
   "source": [
    "<h5>Thank You for reading the artice</h5>\n",
    "<h5>Don't forget to follow me in github and Medium.</h5>\n",
    "<h6>Here is the <a href=''>Code</a></h6> <a href=''>Code</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a86bd93-ce8f-4106-90e7-8823926b69ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
